{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning and Evaluation Notebook\n",
    "\n",
    "In this notebook we go over how to train a Latent Factor Model using Alternating Least Squares (ALS). We also go over tuning the models rank and regularization scaler parameters. Due to the size of the full BYU dataset performing this on analysis on the complete data was computationally infeasible at this time. Instead I performed a simple random sample of only 5% and conducted my analysis on this subset. See the file \"lines_to_np.py\" in the code folder for more details on how this sampling was done. Additionally, see the \"Data Explorarion\" notebook for some explaratory data analysis of the data used here.\n",
    "\n",
    "## 1. Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4783694, 3) (1195924, 3)\n"
     ]
    }
   ],
   "source": [
    "path = \"/Volumes/Samsung_T5/Data/little_array.npy\"\n",
    "\n",
    "df = pd.DataFrame(np.load(path),columns=['steam_id','app_id','interact'])\n",
    "\n",
    "# Need to re-number steam_id's since they get too big for spark\n",
    "minimum = np.min(df['steam_id'])\n",
    "df['user_id'] = df['steam_id'] - minimum\n",
    "df = df[['user_id','app_id','interact']]\n",
    "\n",
    "# Train/Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "print(train.shape,test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Dictionary with Keys = user_ids, Values = list of their hidden games\n",
    "hidden_games = test.groupby('user_id')['app_id'].apply(list)\n",
    "hidden_games = hidden_games.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize the Spark Session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Create Spark Dataframe\n",
    "sp_train = spark.createDataFrame(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Helper Functions for Evaluating Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import RankingMetrics\n",
    "\n",
    "def eval_ALS(model, hidden_games):\n",
    "    \"\"\"\n",
    "    Inputs: model-- a fitted als model, hidden_games-- dictionary with keys = user_id's from test set \n",
    "                    and values = list of the user's hidden (testing) games\n",
    "    Output: list of model's precision at k=10,20,30. This is the average over all users of the fraction of\n",
    "    games in the models top k recommendations that belong to the user's hidden games list\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make Spark Dataframe of users in test set\n",
    "    users = pd.DataFrame({'user_id': list(hidden_games.keys())})\n",
    "    sp_users = spark.createDataFrame(users)\n",
    "    \n",
    "    # Predict Top 30 Games for each test user\n",
    "    preds = model.recommendForUserSubset(sp_users, 30).collect()\n",
    "    \n",
    "    # Make a list of lists, containing each user's predictions and the games we hid in testing\n",
    "    recs_list = []\n",
    "    for user, items in preds:\n",
    "        pred_items = [item.app_id for item in items]\n",
    "        recs_list.append((pred_items, hidden_games[user]))\n",
    "        \n",
    "    # Get average \"hit-rate\" for k =10,20,30\n",
    "    labels = spark.sparkContext.parallelize(recs_list)\n",
    "    metrics = RankingMetrics(labels)\n",
    "    return [metrics.precisionAt(k) for k in [10,20,30]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tuning / Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Rank    RegParam   Precision at k= 10    k= 20        k=30    \n",
      "------------------------------------------------------------\n",
      "10           0.1           0.131           0.107       0.089    \n",
      "10           0.5           0.118           0.097       0.081    \n",
      "10           1.0           0.098           0.081       0.069    \n",
      "10           5.0           0.003           0.003       0.003    \n",
      "10           10.0          0.002           0.002       0.002    \n",
      "15           0.1           0.134           0.111       0.092    \n",
      "15           0.5           0.121           0.098       0.082    \n",
      "15           1.0           0.097           0.082       0.068    \n",
      "15           5.0           0.030           0.023       0.020    \n",
      "15           10.0          0.000           0.001       0.001    \n",
      "20           0.1           0.130           0.112       0.094    \n",
      "20           0.5           0.122           0.099       0.083    \n",
      "20           1.0           0.097           0.081       0.068    \n",
      "20           5.0           0.000           0.000       0.000    \n",
      "20           10.0          0.000           0.000       0.000    \n"
     ]
    }
   ],
   "source": [
    "ranks = [10,15,20]\n",
    "scalers = [0.1,0.5,1,5,10]\n",
    "dash = '-' * 60\n",
    "\n",
    "print(dash)\n",
    "print('{:<6s}{:^12s}{:^20s}{:^12s}{:^12s}'.format('Rank','RegParam', 'Precision at k= 10','k= 20', 'k=30'))\n",
    "print(dash)\n",
    "\n",
    "for r in ranks:\n",
    "    for c in scalers:\n",
    "        als = ALS(rank=r, maxIter=5, regParam=c,\n",
    "          userCol=\"user_id\", itemCol=\"app_id\", implicitPrefs=True,\n",
    "          ratingCol=\"interact\" ,coldStartStrategy=\"drop\")\n",
    "        model = als.fit(sp_train)\n",
    "        precision = eval_ALS(model,hidden_games)\n",
    "        print('{:<10d}{:^10.1f}{:^20.3f}{:^12.3f}{:^12.3f}'.format(r,c,precision[0],precision[1],precision[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Rank    RegParam   Precision at k= 10    k= 20        k=30    \n",
      "------------------------------------------------------------\n",
      "10           0.0           0.128           0.107       0.089    \n",
      "10           0.1           0.131           0.107       0.089    \n",
      "10           0.2           0.130           0.107       0.088    \n",
      "15           0.0           0.124           0.108       0.091    \n",
      "15           0.1           0.130           0.110       0.092    \n",
      "15           0.2           0.136           0.111       0.092    \n"
     ]
    }
   ],
   "source": [
    "ranks = [10,15]\n",
    "scalers = [0.01,0.05,0.25]\n",
    "dash = '-' * 60\n",
    "\n",
    "print(dash)\n",
    "print('{:<6s}{:^12s}{:^20s}{:^12s}{:^12s}'.format('Rank','RegParam', 'Precision at k= 10','k= 20', 'k=30'))\n",
    "print(dash)\n",
    "\n",
    "for r in ranks:\n",
    "    for c in scalers:\n",
    "        als = ALS(rank=r, maxIter=5, regParam=c,\n",
    "          userCol=\"user_id\", itemCol=\"app_id\", implicitPrefs=True,\n",
    "          ratingCol=\"interact\" ,coldStartStrategy=\"drop\")\n",
    "        model = als.fit(sp_train)\n",
    "        precision = eval_ALS(model,hidden_games)\n",
    "        print('{:<10d}{:^10.1f}{:^20.3f}{:^12.3f}{:^12.3f}'.format(r,c,precision[0],precision[1],precision[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Rank    RegParam   Precision at k= 10    k= 20        k=30    \n",
      "------------------------------------------------------------\n",
      "12           0.05          0.132           0.109       0.091    \n",
      "12           0.10          0.133           0.109       0.091    \n",
      "12           0.15          0.134           0.109       0.091    \n",
      "12           0.20          0.135           0.109       0.091    \n",
      "12           0.25          0.135           0.109       0.090    \n",
      "12           0.30          0.133           0.108       0.089    \n",
      "12           0.35          0.131           0.106       0.088    \n",
      "15           0.05          0.130           0.110       0.092    \n",
      "15           0.10          0.134           0.111       0.092    \n",
      "15           0.15          0.136           0.111       0.093    \n",
      "15           0.20          0.136           0.111       0.092    \n",
      "15           0.25          0.136           0.111       0.092    \n",
      "15           0.30          0.135           0.110       0.092    \n",
      "15           0.35          0.133           0.109       0.091    \n",
      "18           0.05          0.129           0.111       0.093    \n",
      "18           0.10          0.132           0.112       0.093    \n",
      "18           0.15          0.136           0.113       0.094    \n",
      "18           0.20          0.137           0.113       0.094    \n",
      "18           0.25          0.138           0.112       0.093    \n",
      "18           0.30          0.137           0.111       0.093    \n",
      "18           0.35          0.135           0.110       0.091    \n"
     ]
    }
   ],
   "source": [
    "ranks = [12,15,18]\n",
    "scalers = [0.05,0.1,0.15,0.2,0.25,0.3,0.35]\n",
    "dash = '-' * 60\n",
    "\n",
    "print(dash)\n",
    "print('{:<6s}{:^12s}{:^20s}{:^12s}{:^12s}'.format('Rank','RegParam', 'Precision at k= 10','k= 20', 'k=30'))\n",
    "print(dash)\n",
    "\n",
    "for r in ranks:\n",
    "    for c in scalers:\n",
    "        als = ALS(rank=r, maxIter=5, regParam=c,\n",
    "          userCol=\"user_id\", itemCol=\"app_id\", implicitPrefs=True,\n",
    "          ratingCol=\"interact\" ,coldStartStrategy=\"drop\")\n",
    "        model = als.fit(sp_train)\n",
    "        precision = eval_ALS(model,hidden_games)\n",
    "        print('{:<10d}{:^10.2f}{:^20.3f}{:^12.3f}{:^12.3f}'.format(r,c,precision[0],precision[1],precision[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the eval_ALS function I trained and evaluated several models with different ranks and regularization parameters. Recall that the rank is the dimension of the feature/preference vector for each item/user and the regularization parameter is the value we called $\\lambda$ in the previous notebook, which was used to scale the $\\sum_{i} ||U_i||^2 +  \\sum_j ||V_j||^2$ term.\n",
    "\n",
    "By testing and adjusting, I determined that the best choice of parameter was rank =18 and regParam = 0.25, which gave us a precision at $k$ of $13.8\\%$, i.e. about 1 in 7 of our recommendations was actually in the test set. This is pretty good when you consider the sparsity of the matrix. For a baseline comparisson, let's see how we would do if we just recommended the 10 most popular games to every user. We computed these games in the \"Data Exploration\" notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0887993750305153\n"
     ]
    }
   ],
   "source": [
    "# Recommend Popular Games Baseline\n",
    "\n",
    "top_ten_apps = [340,240,320,220,400,10,550,223530,30,40]\n",
    "\n",
    "top_ten_recs = [(top_ten_apps, hidden_games[user]) for user in list(hidden_games.keys())]\n",
    "labels = spark.sparkContext.parallelize(top_ten_recs)\n",
    "\n",
    "metrics = RankingMetrics(labels)\n",
    "\n",
    "print(metrics.precisionAt(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that our model significantly outperforms the baseline model which just recommend the top 10 most popular games to every user, which got a precision at 10 of just $8.8\\%$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
